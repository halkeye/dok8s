---
serviceAccounts:
  server:
    create: true
    annotations: {}

alertmanager:
  strategy:
    type: Recreate
  persistentVolume:
    storageClass: do-block-storage
  baseURL: https://alertmanager.g4v.dev/
  ingress:
    enabled: true
    hosts:
      - alertmanager.g4v.dev
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/auth-signin: "https://vouch.g4v.dev/login?url=$scheme://$http_host$request_uri&lasso-failcount=$auth_resp_failcount&X-Vouch-Token=$auth_resp_jwt&error=$auth_resp_err"
      nginx.ingress.kubernetes.io/auth-url: https://vouch.g4v.dev
      nginx.ingress.kubernetes.io/auth-response-headers: X-Vouch-User
      nginx.ingress.kubernetes.io/auth-snippet: |
        # these return values are used by the @error401 call
        auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
        auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
        auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;

server:
  baseURL: https://prometheus.g4v.dev/
  strategy:
    type: Recreate
  persistentVolume:
    storageClass: do-block-storage
  ingress:
    enabled: true
    hosts:
      - prometheus.g4v.dev
    ingressClassName: "nginx"
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/auth-signin: "https://vouch.g4v.dev/login?url=$scheme://$http_host$request_uri&lasso-failcount=$auth_resp_failcount&X-Vouch-Token=$auth_resp_jwt&error=$auth_resp_err"
      nginx.ingress.kubernetes.io/auth-url: https://vouch.g4v.dev
      nginx.ingress.kubernetes.io/auth-response-headers: X-Vouch-User
      nginx.ingress.kubernetes.io/auth-snippet: |
        # these return values are used by the @error401 call
        auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
        auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
        auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;
  podAnnotations:
    checksum.tailscale: '{{ .Values.tailscaleApiKey | sha256sum | quote }}'
  sidecarContainers:
    tailscale:
      image: docker.io/mvisonneau/tailscale:v1.28.0
      envFrom:
        - secretRef:
            name: tailscale-prometheus-config
      env:
        - { name: TAILSCALE_HOSTNAME, value: prometheus }
      volumeMounts:
        - name: lib-modules
          mountPath: /lib/modules
      securityContext:
        runAsGroup: 0
        runAsNonRoot: false
        runAsUser: 0
        capabilities:
          add:
            - NET_ADMIN
  extraVolumes:
    - name: lib-modules
      hostPath:
        path: /lib/modules

extraManifests:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: tailscale-prometheus-config
    type: Opaque
    stringData:
      TAILSCALE_AUTH_KEY: "{{ .Values.tailscaleApiKey }}"

pushgateway:
  baseURL: https://pushgateway.g4v.dev/
  strategy:
    type: Recreate
  persistentVolume:
    storageClass: do-block-storage
  #ingress:
  #  enabled: true
  #  hosts:
  #    - pushgateway.g4v.dev
  #  ingressClassName: "nginx"
  #  annotations:
  #    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
#
#
# The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
# prometheus-pushgateway.default.svc.cluster.local

# Get the PushGateway URL by running these commands in the same shell:
#   export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
#   kubectl --namespace default port-forward $POD_NAME 9091
#
kubeStateMetrics:
  enabled: false

serverFiles:
  alerting_rules.yml:
    groups:
      - name: Instances
        rules:
          - alert: PrometheusJobMissing
            expr: absent(up{job="prometheus"})
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Prometheus job missing (instance {{ $labels.instance }})
              description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: PrometheusTargetMissing
            expr: up == 0
            for: 15m
            labels:
              severity: critical
            annotations:
              summary: Prometheus target missing (instance {{ $labels.instance }})
              description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: PrometheusNotificationsBacklog
            expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Prometheus notifications backlog (instance {{ $labels.instance }})
              description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: PrometheusTargetMissingWithWarmupTime
            expr: sum by (instance, job) ((up == 0) * on (instance) group_right(job) (node_time_seconds - node_boot_time_seconds > 600))
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Prometheus target missing with warmup time (instance {{ $labels.instance }})
              description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: PrometheusConfigurationReloadFailure
            expr: prometheus_config_last_reload_successful != 1
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Prometheus configuration reload failure (instance {{ $labels.instance }})
              description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: TraefikServiceDown
            expr: count(traefik_service_server_up) by (service) == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Traefik service down (instance {{ $labels.instance }})
              description: "All Traefik services are down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: JenkinsOffline
            expr: jenkins_node_offline_value > 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Jenkins offline (instance {{ $labels.instance }})
              description: "Jenkins offline: `{{$labels.instance}}` in realm {{$labels.realm}}/{{$labels.env}} ({{$labels.region}})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: JenkinsHealthcheck
            expr: jenkins_health_check_score < 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Jenkins healthcheck (instance {{ $labels.instance }})
              description: "Jenkins healthcheck score: {{$value}}. Healthcheck failure for `{{$labels.instance}}` in realm {{$labels.realm}}/{{$labels.env}} ({{$labels.region}})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: JenkinsOutdatedPlugins
            expr: sum(jenkins_plugins_withUpdate) by (instance) > 3
            for: 1d
            labels:
              severity: warning
            annotations:
              summary: Jenkins outdated plugins (instance {{ $labels.instance }})
              description: "{{ $value }} plugins need update\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          #- alert: JenkinsBuildsHealthScore
          #  expr: default_jenkins_builds_health_score < 1
          #  for: 0m
          #  labels:
          #    severity: warning
          #  annotations:
          #    summary: Jenkins builds health score (instance {{ $labels.instance }})
          #    description: "Healthcheck failure for `{{$labels.instance}}` in realm {{$labels.realm}}/{{$labels.env}} ({{$labels.region}})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: InstanceDown
            expr: up == 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: "{{ $labels.instance }} of job {{ $labels.job }}/{{ $labels.kubernetes_pod_name }} has been down for more than 5 minutes."
              summary: "Instance {{ $labels.instance }} down"
          - alert: Pods Down
            expr: sum(kube_pod_status_phase{phase !~ "Running|Succeeded"}) by (phase) > 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: "Bad pods for more than than 5 minutes."
              summary: "Instance {{ $labels.instance }} down"
          - alert: Unused PV
            expr: sum(kube_persistentvolume_status_phase{phase!~"Bound"}) by (phase) > 0
            for: 5m
            labels:
              severity: page
            annotations:
              description: "Unused pv for more than than 5 minutes."
              summary: "Unused PV"
          - alert: Broken PVC
            expr: sum(kube_persistentvolumeclaim_status_phase{phase!~"Bound"}) by (phase) > 0
            for: 5m
            labels:
              severity: page
            annotations:
              summary: "Broken PVC(s)"
          - alert: Matrix Bridge is down
            expr: bridge_connected == 0
            for: 5m
            labels:
              severity: page
            annotations:
              summary: "Matrix bridge is down"
          - alert: ProwlarrSystemMessages
            expr: prowlarr_system_health_issues
            for: 1hr
            labels:
              severity: warning
            annotations:
              summary: Prowlarr System Message - {{$value.message}}
